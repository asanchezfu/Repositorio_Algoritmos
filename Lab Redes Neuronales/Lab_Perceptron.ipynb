{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Laboratorio Sobre el Perceptrón\n",
        "\n"
      ],
      "metadata": {
        "id": "r5YoNb3rlXPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionamos el dataset de [cancer de mama](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=breast%20cancer ) de sklearn que contiene 569 ejemplos y dos clases para su clasificación, definiendo si el tumor es maligno o benigno."
      ],
      "metadata": {
        "id": "AKzPzpA6Wf3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P9LgZADP95wE"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#we import the data\n",
        "cancer = datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "#determine the number of registers\n",
        "number_of_registers = len(cancer[0])\n",
        "#determine the number of training cases\n",
        "training_cases = int(number_of_registers*0.8)\n",
        "#and thus, the number of test registers\n",
        "test_cases = number_of_registers - training_cases\n",
        "#we select  indexes at random, as many of the training set size we desire\n",
        "selection_training = np.random.choice(number_of_registers,training_cases)\n",
        "#we take the data at these indexes\n",
        "training_set = cancer[0].iloc[selection_training].values, np.where(cancer[1][selection_training]==1, 1, -1)\n",
        "#we take the remainder indexes\n",
        "test_selection = np.array([i for i in range(number_of_registers) if i not in selection_training])\n",
        "#and take those points of data in order to create a test set\n",
        "test_set = cancer[0].iloc[test_selection].values, np.where(cancer[1][test_selection]==1, 1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train y test con dos variables"
      ],
      "metadata": {
        "id": "PMiQ17EHbV_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_2 = cancer[0].iloc[selection_training][['mean texture','mean symmetry']].values, np.where(cancer[1][selection_training]==1, 1, -1)\n",
        "test_2 = cancer[0].iloc[test_selection][['mean texture','mean symmetry']].values, np.where(cancer[1][test_selection]==1, 1, -1)"
      ],
      "metadata": {
        "id": "MoOYvQm8bZIR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cancer[0].columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4svIHGKfJ7Sr",
        "outputId": "986b6ea7-2304-477d-b239-01051d2783bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
            "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
            "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
            "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
            "       'smoothness error', 'compactness error', 'concavity error',\n",
            "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
            "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
            "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
            "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we create our perceptron\n",
        "class Perceptron:\n",
        "    def __init__(self, eta=0.1, n_iter=10):\n",
        "        self.eta = float(eta)\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # inicializar los pesos en 0\n",
        "        self.w = np.zeros(len(X[1])+1)\n",
        "        #vector de errores acumulados\n",
        "        self.errors = []\n",
        "\n",
        "        #ciclo de entrenamiento\n",
        "        for i in range(self.n_iter):\n",
        "            errors = 0\n",
        "            for x_i, target in zip(X,y):\n",
        "                #calcular el nuevo valor de los pesos\n",
        "                delta_w = np.array((target - self.predict(x_i)) * self.eta)\n",
        "                #actualizar el valor de los pesos\n",
        "                self.w[1:] += delta_w * x_i\n",
        "                #actualizar el valor del bias\n",
        "                self.w[0] += delta_w\n",
        "                if (delta_w!=0):\n",
        "                    errors += 1\n",
        "            self.errors.append(errors)\n",
        " \n",
        "\n",
        "    def predict(self, X):\n",
        "        #combinacion lineal, w[0] = bias\n",
        "        v = np.dot( X, self.w [1:]) + self.w[0]\n",
        "        #funcion de activación \n",
        "        if v > 0.0:\n",
        "            return 1\n",
        "        return -1"
      ],
      "metadata": {
        "id": "SauGlRNGCnes"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento y predicción con todas las variables del dataset"
      ],
      "metadata": {
        "id": "CcObBjXicp_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = Perceptron( n_iter = 5000)\n",
        "p.train(training_set[0], training_set[1])\n",
        "aciertos = list()\n",
        "for i, j in zip(test_set[0],test_set[1]):\n",
        "  aciertos.append(p.predict(i) == j)\n",
        "print(sum(aciertos)/len(aciertos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ypnuFE1L8xI",
        "outputId": "9036a620-7223-492d-86f2-539325782efa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento y predicción con 2 variables del dataset"
      ],
      "metadata": {
        "id": "5DiD7NgicvZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = Perceptron( n_iter = 5000)\n",
        "p.train(train_2[0], train_2[1])\n",
        "aciertos = list()\n",
        "for i, j in zip(test_2[0],test_2[1]):\n",
        "  aciertos.append(p.predict(i) == j)\n",
        "print(sum(aciertos)/len(aciertos))"
      ],
      "metadata": {
        "id": "2odtcuwCb1A7",
        "outputId": "fe8f0005-8d7f-4762-8d17-31c9fbf16c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7154150197628458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Complejidad\n",
        "\n",
        "La complejidad de este algoritmo depende principalmente de la cantidad de parámetros que usaremos y sus iteraciones. Sea $n$ el número de parámetros e $i$ el número de iteraciones.Para calcular delta_w es necesario ralizar tantas multiplicaciones como parámetros, el nuevo w requiere esta misma cantidad de multiplicaciones (como es stándar, no contamos las sumas). Esta operación se repite tantas veces como datos en el set de entrenamiento, llamemos a este número $m$. Por último, todo esto se repite tantas veces como iteraciones tenemos. Esto es: $O(imn)$. Si asumimos $i=n=m$ tenemos $O(n^3)$."
      ],
      "metadata": {
        "id": "kdIofVFT1f6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Separabilidad de los datos"
      ],
      "metadata": {
        "id": "b9Y41tMfcniy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando se usan todas las variables es muy complejo definir la separabilidad de los datos. Sin embargo, por el índice que se obtuvo al evaluar las predicciones del perceptrón se puede afirmar que los datos deben ser separables."
      ],
      "metadata": {
        "id": "KxLtv5vhdIii"
      }
    }
  ]
}